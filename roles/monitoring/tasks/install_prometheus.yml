---
- name: Label project namespaces for monitoring
  when: prometheus_enabled
  kubernetes.core.k8s:
    state: present
    api_version: v1
    kind: Namespace
    name: "{{ item.name }}"
    definition:
      metadata:
        labels:
          monitoring: "{{ item.monitoring | default('enabled') | string }}"
  loop: "{{ self_saas_projects }}"
  loop_control:
    label: "{{ item.name }}"

- name: Label Prometheus release namespace for monitoring
  when: prometheus_enabled
  kubernetes.core.k8s:
    state: present
    api_version: v1
    kind: Namespace
    name: "{{ monitoring_namespace }}"
    definition:
      metadata:
        labels:
          monitoring: "enabled"

- name: Ensure `kube-system` namespace is monitored
  when: prometheus_enabled
  kubernetes.core.k8s:
    state: present
    api_version: v1
    kind: Namespace
    name: kube-system
    definition:
      metadata:
        labels:
          monitoring: "enabled"

- name: Install kube-prometheus-stack (Prometheus Operator) using Helm
  when: prometheus_enabled
  kubernetes.core.helm:
    name: "{{ kube_prometheus_release_name }}"
    chart_ref: "{{ kube_prometheus_chart_ref }}"
    chart_version: "{{ kube_prometheus_chart_version }}"
    release_namespace: "{{ monitoring_namespace }}"
    create_namespace: false
    values:
      # Configure embedded Grafana in the kube-prometheus-stack using role variables
      grafana:
        enabled: "{{ grafana_enabled | default(false) | bool }}"
        persistence:
          enabled: true
          storageClassName: "{{ grafana_storage_class }}"
          accessModes: ["ReadWriteOnce"]
          size: "{{ grafana_storage_size }}"
        adminUser: "{{ grafana_admin_user }}"
        adminPassword: "{{ grafana_admin_password }}"
        sidecar:
          dashboards:
            enabled: "{{ grafana_sidecar_enabled | default(false) | bool }}"
            label: "{{ grafana_sidecar_label }}"
            searchNamespace: "{{ grafana_sidecar_search_namespace }}"
        # Note: grafana datasource provisioning is handled by a separate
        # Kubernetes ConfigMap task below to avoid Helm template type errors.
      prometheusOperator:
        # ensure CRDs are created by the chart
        createCustomResource: true
        # Limit operator's interaction to the release namespace and project namespaces
        # so the operator will watch/manage ServiceMonitor/PrometheusRule CRs in those
        # namespaces. `additional` is populated from `self_saas_projects` names.
        namespaces:
          releaseNamespace: true
          additional: "{{ self_saas_projects | map(attribute='name') | list }}"
      prometheus:
        # Ensure a deterministic ServiceAccount name so RoleBindings can reference it
        serviceAccount:
          create: true
          name: "{{ kube_prometheus_prometheus_sa_name }}"
        prometheusSpec:
          # Add a cluster external label so dashboards that filter by 'cluster' work
          externalLabels:
            cluster: "{{ prometheus_cluster_name | default('self-host-saas-k3s') }}"
          # External URL for Prometheus web UI. Set to localhost for local port-forwarding.
          externalUrl: "http://localhost:9090/"
          # Ensure each scraped target receives a 'cluster' label so metrics include it
          relabelConfigs:
            - targetLabel: cluster
              replacement: "{{ prometheus_cluster_name | default('self-host-saas-k3s') }}"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: "{{ prometheus_storage_class }}"
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: "{{ prometheus_storage_size }}"
          # Allow configuring which namespaces Prometheus will search for ServiceMonitors
          # Use a label-based namespace selector so the Prometheus CR is valid.
          # We label target namespaces with `monitoring=enabled` in the
          # `setup_namespace_monitoring_rbac.yml` task above and select by that label.
          serviceMonitorNamespaceSelector:
            matchLabels:
              monitoring: "enabled"
      alertmanager:
        enabled: "{{ prometheus_alertmanager_enabled | default(true) | bool }}"
        alertmanagerSpec:
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: "{{ prometheus_storage_class }}"
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: "{{ prometheus_alertmanager_storage_size }}"
          # External URL for Alertmanager web UI. Set to localhost for local port-forwarding.
          externalUrl: "http://localhost:9093/"
    wait: true
    wait_timeout: "{{ helm_install_timeout }}"

- name: Gather per-project Pushgateway credentials from project secrets
  when: prometheus_enabled and pushgateway_enabled
  block:
    - name: Validate Pushgateway credentials are provided for enabled projects
      ansible.builtin.assert:
        that:
          - "item.pushgateway.password is defined"
        fail_msg: "Project {{ item.name }} must define pushgateway.password in inventory when pushgateway.enabled is true."
      loop: "{{ self_saas_projects }}"
      loop_control:
        label: "{{ item.name }}"
      when: item.pushgateway is defined and (item.pushgateway.enabled | default(false) | bool)

    - name: Initialize pushgateway_basic_auth_users mapping
      ansible.builtin.set_fact:
        pushgateway_basic_auth_users: {}
      when: pushgateway_basic_auth_users is not defined

    - name: Build pushgateway_basic_auth_users mapping from `self_saas_projects`
      ansible.builtin.set_fact:
        pushgateway_basic_auth_users: "{{ pushgateway_basic_auth_users | default({}) | combine({ (item.pushgateway.user | default(item.name)): item.pushgateway.password }) }}"
      loop: "{{ self_saas_projects }}"
      loop_control:
        label: "{{ item.name }}"
      when: item.pushgateway.enabled | default(false)

- name: Install Pushgateway (for job metrics) using Helm
  when: prometheus_enabled and pushgateway_enabled
  kubernetes.core.helm:
    name: "{{ pushgateway_release_name }}"
    chart_ref: "{{ pushgateway_chart_ref }}"
    chart_version: "{{ pushgateway_chart_version }}"
    release_namespace: "{{ monitoring_namespace }}"
    create_namespace: false
    # Before installing Pushgateway, collect per-project credentials from created secrets
    # and supply them to the chart via `webConfiguration.basicAuthUsers`.
    values:
      # Force a stable service name so we can reference it from app secrets
      fullnameOverride: "{{ pushgateway_service_name }}"
      service:
        type: ClusterIP
        port: "{{ pushgateway_service_port }}"
      webConfiguration:
        # populated below by a task that reads secrets; placeholder if empty
        basicAuthUsers: "{{ pushgateway_basic_auth_users }}"
    wait: true
    wait_timeout: "{{ helm_install_timeout | default('300') }}"

- name: Create per-namespace RBAC for Prometheus discovery
  when: prometheus_enabled
  block:
    - name: Create Role for Prometheus discovery in project namespaces
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: Role
          metadata:
            name: prometheus-discovery
            namespace: "{{ item.name }}"
          rules:
            - apiGroups: [""]
              resources:
                ["services", "endpoints", "pods", "secrets", "configmaps"]
              verbs: ["get", "list", "watch"]
            - apiGroups: ["monitoring.coreos.com"]
              resources: ["servicemonitors", "podmonitors"]
              verbs: ["get", "list", "watch"]
      loop: "{{ self_saas_projects }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Create RoleBinding in project namespaces to bind Prometheus ServiceAccount
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: RoleBinding
          metadata:
            name: prometheus-discovery-binding
            namespace: "{{ item.name }}"
          subjects:
            - kind: ServiceAccount
              name: "{{ kube_prometheus_prometheus_sa_name }}"
              namespace: "{{ monitoring_namespace }}"
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: Role
            name: prometheus-discovery
      loop: "{{ self_saas_projects }}"
      loop_control:
        label: "{{ item.name }}"

- name: Ensure Grafana 'default' datasource exists as a ConfigMap
  when: prometheus_enabled and grafana_sidecar_enabled
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: "grafana-datasource-default"
        namespace: "{{ monitoring_namespace }}"
        labels:
          grafana_datasource: "1"
      data:
        datasource.yaml: |-
          apiVersion: 1
          datasources:
          - name: "Prometheus"
            type: prometheus
            uid: prometheus
            url: http://{{ kube_prometheus_release_name }}-kube-prome-prometheus.{{ monitoring_namespace }}:9090/
            access: proxy
            isDefault: true
          - name: "default"
            type: prometheus
            uid: default
            url: http://{{ kube_prometheus_release_name }}-kube-prome-prometheus.{{ monitoring_namespace }}:9090/
            access: proxy
            isDefault: false
          {% if loki_enabled and grafana_sidecar_enabled %}
          - name: "Loki"
            type: loki
            uid: loki
            url: "{{ loki_service_url }}"
            access: proxy
            isDefault: false
            jsonData:
              httpHeaderName1: "X-Scope-OrgID"
            secureJsonData:
              httpHeaderValue1: "fluent-bit"
          {% endif %}
