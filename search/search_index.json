{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Self-Host SaaS K3s","text":"<p>Welcome to the Self-Host SaaS K3s documentation. This project provides a complete solution for self-hosting your SaaS applications using K3s, a lightweight Kubernetes distribution.</p>"},{"location":"#overview","title":"Overview","text":"<p>Self-Host SaaS K3s is designed to help you deploy and manage your SaaS applications on your own infrastructure. It provides:</p> <ul> <li>A lightweight Kubernetes cluster using K3s</li> <li>Container registry with Harbor</li> <li>Persistent storage with Longhorn</li> <li>Automated backups to Backblaze</li> <li>CI/CD integration with GitHub Actions</li> <li>Monitoring and logging stack</li> <li>Security-first approach</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Prerequisites - System requirements and preparation</li> <li>Installation - Step-by-step installation guide</li> <li>Security - Security configuration and best practices</li> </ol>"},{"location":"#core-components","title":"Core Components","text":"<ol> <li>K3s - Managing your K3s cluster</li> <li>Harbor - Container registry setup and usage</li> <li>Longhorn - Storage and backup management</li> </ol>"},{"location":"#application-management","title":"Application Management","text":"<ol> <li>Applications - CI/CD and managing applications</li> <li>Monitoring - Setting up monitoring and alerts </li> </ol>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker </li> <li>Contributing Guide</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"applications/","title":"Applications Guide","text":"<p>This guide explains how to deploy and manage applications in your Self-Host SaaS K3s cluster.</p>"},{"location":"applications/#overview","title":"Overview","text":"<p>The deployment and management of applications in this cluster follows a GitOps approach using Argo CD for Continuous Deployment (CD) and GitHub Actions for Continuous Integration (CI).</p>"},{"location":"applications/#example-application","title":"Example Application","text":"<p>For reference, you can check out the example application at: https://github.com/humansoftware/example_self_hosted_saas_app</p>"},{"location":"applications/#configuring-your-applications","title":"Configuring your applications","text":"<p>To deploy applications to your cluster, you need to configure them in the cluster's configuration:</p> <ol> <li>Edit the file <code>group_vars/all/secrets.yml</code> to specify which applications to deploy</li> <li>Add your application repositories to the <code>self_saas_projects</code> list</li> <li>Each repository should be specified in the format: <code>owner/repo-name</code></li> </ol> <p>For an example configuration, see secrets.example.yml</p>"},{"location":"applications/#github-authentication","title":"GitHub Authentication","text":"<p>The cluster needs access to your GitHub repositories for two purposes: 1. Argo CD needs to clone repositories to deploy applications 2. GitHub Actions need to interact with GitHub's API</p> <p>To enable this access: 1. Create a GitHub Personal Access Token (PAT) with appropriate permissions:    - <code>repo</code> scope for private repositories    - <code>workflow</code> scope for GitHub Actions 2. Add this token to the <code>github_pat</code> field in <code>group_vars/all/secrets.yml</code></p> <p>The token will be securely stored and used by both Argo CD and GitHub Actions runners.</p>"},{"location":"applications/#application-structure","title":"Application Structure","text":""},{"location":"applications/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<ul> <li>GitHub Actions is used for CI</li> <li>Actions are defined in your application repository as you would normally do in GitHub</li> <li>Key differences from standard GitHub Actions:</li> <li>Actions run in your self-hosted cluster instead of GitHub runners</li> <li>Pull request statuses and logs are updated the same way as with native GitHub runners</li> </ul>"},{"location":"applications/#docker-image-publishing","title":"Docker Image Publishing","text":"<p>To publish Docker images to Harbor:</p> <ol> <li>Set up a GitHub secret for your application containing the Harbor admin password</li> <li>This password should match what you configured in the secrets configuration</li> <li>Use the following credentials for Docker login:</li> <li>Username: <code>admin</code></li> <li>Password: The Harbor admin password secret you configured</li> <li>host: harbor.local</li> </ol>"},{"location":"applications/#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":""},{"location":"applications/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ul> <li>Applications must include Kubernetes Custom Resource Definitions (CRDs) in the <code>deployment</code> folder</li> <li>These configurations define your application's components:</li> <li>Jobs</li> <li>Services</li> <li>Pods</li> <li>Other Kubernetes resources</li> <li>The configurations can reference Docker images published by your CI pipelines</li> </ul>"},{"location":"applications/#kustomize-integration","title":"Kustomize Integration","text":"<ul> <li>Argo CD uses Kustomize for managing Kubernetes configurations</li> <li>Each application should include a <code>kustomization.yaml</code> file that:</li> <li>References the Kubernetes manifests</li> <li>Defines common labels and annotations</li> <li>Manages environment-specific configurations</li> <li>For more information about Kustomize, see the official documentation</li> <li>To learn how Argo CD integrates with Kustomize, refer to the Argo CD Kustomize documentation</li> </ul>"},{"location":"applications/#argo-cd-ui","title":"Argo CD UI","text":"<p>Execute the following command to create a port-forward for the Argo CD UI:</p> <pre><code>kubectl -n argocd port-forward svc/argocd-server 8082:80\n</code></pre> <p>Visit http://localhost:8082 and log in with admin and the password you specified in secrets.</p>"},{"location":"applications/#best-practices","title":"Best Practices","text":"<ol> <li>Keep your Kubernetes configurations in the <code>deployment</code> folder organized and well-documented</li> <li>Ensure your GitHub Actions workflows are properly configured for self-hosted runners</li> <li>Securely manage your Harbor credentials using GitHub secrets</li> <li>Follow the example application structure for consistency</li> </ol>"},{"location":"harbor/","title":"Harbor Container Registry","text":"<p>Harbor is an open-source container registry that secures, stores, and signs container images. It's chosen for this project because it provides enterprise-grade features like vulnerability scanning, image signing, and role-based access control.</p> <p>All services and containers in k3s will load their images from harbor and CI/CD will publish and refer to images in harbor as well.</p>"},{"location":"harbor/#configuration-variables","title":"Configuration Variables","text":"<p>The following variables can be customized in <code>group_vars/all/secrets.yml</code>:</p> Variable Description Default <code>harbor_admin_password</code> Admin password for Harbor UI \"ChangeMe123!\" <code>harbor_registry_size</code> Size of registry storage \"6Gi\""},{"location":"harbor/#accessing-harbor","title":"Accessing Harbor","text":""},{"location":"harbor/#ui-access","title":"UI Access","text":"<p>Only access through ssh tunnel is set, as exposing harbor UI port would pose a security risk. </p> <ol> <li>Run the following command on your local machine (with <code>kubectl</code> configured to access your cluster):</li> </ol> <pre><code>kubectl -n kube-system port-forward svc/traefik 8081:80\n# kubectl port-forward svc/harbor-portal -n harbor 8081:80 would point directly to the service\n</code></pre> <ol> <li> <p>Access the Harbor UI at http://localhost:8081</p> </li> <li> <p>Default username: <code>admin</code></p> </li> <li>Password: Set in <code>harbor_admin_password</code></li> </ol> <p>The UI can be useful to see which images are built, restore backups, set tags, etc.</p>"},{"location":"harbor/#docker-login","title":"Docker Login","text":"<p>Using the harbor password secret, it's possible to execute <code>docker login</code> in CI/CD and publish docker images built by your applications.</p>"},{"location":"harbor/#verifying-installation","title":"Verifying Installation","text":"<p>To verify Harbor is correctly installed:</p> <ol> <li>Check Harbor resources:</li> </ol> <pre><code>kubectl get all -n harbor\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":"<p>This guide will walk you through the installation of Self-Host SaaS K3s on your server.</p>"},{"location":"installation/#preparation","title":"Preparation","text":"<ol> <li>Ensure you have completed all prerequisites</li> <li>Prepare your server using cloud-init</li> <li>Have your configuration files ready</li> </ol>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/humansoftware/self-host-saas-k3s.git\ncd self-host-saas-k3s\n</code></pre>"},{"location":"installation/#2-configure-secrets","title":"2. Configure Secrets","text":"<ol> <li> <p>Copy the secrets template:    <code>bash    cp group_vars/all/secrets.example.yml group_vars/all/secrets.yml</code></p> </li> <li> <p>Edit <code>group_vars/all/secrets.yml</code> with your values. See secrets.example.yml for the complete list of required properties.</p> </li> </ol>"},{"location":"installation/#3-update-inventory","title":"3. Update Inventory","text":"<ol> <li> <p>Copy the sample inventory:    <code>bash    cp inventory.sample.yml inventory.yml</code></p> </li> <li> <p>Edit <code>inventory.yml</code> with your server details. See inventory.sample.yml for the complete example.</p> </li> </ol> <p>Make sure to:    - Update <code>host_public_ip</code> with your server's IP address    - Set <code>ansible_port</code> to 2222 (the new SSH port configured by cloud-init)</p>"},{"location":"installation/#4-install-dependencies","title":"4. Install Dependencies","text":"<pre><code>ansible-galaxy install -r requirements.yml\n</code></pre>"},{"location":"installation/#5-run-the-playbook","title":"5. Run the Playbook","text":"<pre><code>ansible-playbook -i inventory.yml playbook.yml \n</code></pre>"},{"location":"installation/#note-enabling-ssh-tunnel-persistence","title":"Note: Enabling SSH Tunnel Persistence","text":"<p>If you want the SSH tunnel to persist even after you log out, you must enable user lingering on your system. This allows user services (like the k3s-tunnel systemd user service) to continue running after logout.</p> <p>To enable lingering for your user, run the following command as root:</p> <pre><code>sudo loginctl enable-linger &lt;your-username&gt;\n</code></pre> <p>Replace <code>&lt;your-username&gt;</code> with your actual Linux username. This step is not performed automatically by the playbook and must be done manually if persistent tunnels are required.</p>"},{"location":"installation/#using-a-separate-repository-for-customization-recommended","title":"Using a Separate Repository for Customization (Recommended)","text":"<p>For production or custom environments, it is best practice to keep your sensitive and environment-specific files (such as <code>inventory.yml</code>, <code>secrets.yml</code>, and <code>cloud-init.yaml</code>) in a separate private repository. This keeps your secrets and customizations out of the main project and makes upgrades and collaboration safer.</p>"},{"location":"installation/#example-directory-structure-for-your-private-repo","title":"Example Directory Structure for Your Private Repo","text":"<pre><code>human_infra/\n\u251c\u2500\u2500 cloud-init.prod.yaml\n\u251c\u2500\u2500 inventory.prod.yml\n\u251c\u2500\u2500 secrets.prod.yml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"installation/#running-ansible-with-your-custom-files","title":"Running Ansible with Your Custom Files","text":"<p>From your private repo (e.g., <code>human_infra</code>), run:</p> <pre><code>ansible-playbook -i inventory.prod.yml \\\n  ../self-host-saas-k3s/playbook.yml \\\n  -e @secrets.prod.yml \\\n  -e cloud_init_file=cloud-init.prod.yaml\n</code></pre> <ul> <li>Adjust the paths as needed for your environment.</li> <li>The <code>cloud_init_file</code> variable can be used in your playbook or roles to reference the correct cloud-init file.</li> </ul> <p>Why use this pattern? - Keeps secrets and sensitive configs out of the main repo - Makes it easier to upgrade the main project without overwriting your custom files - Lets you share the main repo publicly, but keep your infrastructure private</p> <p>See the <code>samples/</code> folder for templates.</p>"},{"location":"installation/#verification","title":"Verification","text":"<p>Each component has its own verification steps in its documentation. After installation, verify that all core components are working correctly.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>See the documentation index for additional guides and next steps.</p>"},{"location":"k3s/","title":"K3s","text":"<p>K3s is a lightweight, certified Kubernetes distribution designed for production workloads in resource-constrained environments and edge locations. It is easy to install, simple to operate, and optimized for both development and production use cases.</p>"},{"location":"k3s/#why-k3s","title":"Why K3s?","text":"<ul> <li>Lightweight: Minimal resource requirements compared to standard Kubernetes.</li> <li>Simplicity: Single binary, easy upgrades, and reduced operational overhead.</li> <li>Production Ready: CNCF certified, supports all standard Kubernetes APIs.</li> <li>Great for Edge/IoT: Designed for environments where resources are limited.</li> </ul>"},{"location":"k3s/#k3s-installation-options","title":"K3s Installation Options","text":"<p>The K3s installation in this project uses specific options to fit the deployment requirements:</p> <ul> <li>Docker Registry Integration: Docker is used as a local registry to store and distribute container images within the cluster.</li> <li>Traefik as Ingress Controller: Traefik is enabled by default to manage ingress resources.</li> <li>Automatic TLS with Cert-Manager: Cert-Manager is installed to automate TLS certificate management using Let's Encrypt.</li> </ul> <p>You can review or customize these options in the Ansible role variables and installation scripts.  </p>"},{"location":"k3s/#variables-to-customize-in-the-k3s-role","title":"Variables to Customize in the K3s Role","text":"<p>The following variables can be customized for the K3s role. These are typically set in your Ansible group or host variables:</p> Variable Description Example Value <code>k3s_token</code> Cluster join token for K3s nodes <code>\"REPLACE_WITH_YOUR_K3S_TOKEN\"</code> <code>k3s_local_kubeconfig</code> Path to local kubeconfig file <code>\"{{ lookup('env', 'HOME') }}/.kube/config\"</code> <code>letsencrypt_email</code> Email for Let's Encrypt certificate registration <code>\"your@email.com\"</code> <p>See group_vars/all/secrets.example.yml for a full list of secrets and example values.</p>"},{"location":"k3s/#how-to-verify-k3s-installation","title":"How to Verify K3s Installation","text":"<p>After running the Ansible playbook, verify your K3s installation:</p> <ol> <li>Check Node Status:</li> </ol> <pre><code>kubectl get nodes\n</code></pre> <ol> <li>List All resources in All Namespaces:</li> </ol> <pre><code>kubectl get all --all-namespaces\n</code></pre> <ol> <li>Inspect Cluster Info:</li> </ol> <pre><code>kubectl cluster-info\n</code></pre>"},{"location":"k3s/#cert-manager","title":"Cert-Manager","text":"<p>Cert-Manager is used to automate the management and issuance of TLS certificates in your cluster.</p> <ul> <li>Check Cert-Manager Status:</li> </ul> <pre><code>kubectl get pods -n cert-manager\n</code></pre> <ul> <li>List Issuers and Certificates:</li> </ul> <pre><code>kubectl get issuers,clusterissuers -A\nkubectl get certificates -A\n</code></pre>"},{"location":"k3s/#ingress-controller","title":"Ingress Controller","text":"<p>Traefik is the default ingress controller bundled with K3s. It automatically manages ingress resources and routes external traffic to your services.</p> <ul> <li>Check Traefik Pods:</li> </ul> <pre><code>kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre> <ul> <li>List Ingress Resources:</li> </ul> <pre><code>kubectl get ingress --all-namespaces\n</code></pre> <ul> <li>Check Ingress Controller Pods:</li> </ul> <pre><code>kubectl get pods -n ingress-nginx\n</code></pre> <ul> <li>List Ingress Resources:</li> </ul> <pre><code>kubectl get ingress --all-namespaces\n</code></pre>"},{"location":"k3s/#additional-resources","title":"Additional Resources","text":"<ul> <li>K3s Documentation</li> <li>Cert-Manager Documentation</li> <li>Kubernetes Ingress Docs</li> </ul>"},{"location":"longhorn/","title":"Longhorn Storage","text":"<p>Longhorn provides persistent storage for your Kubernetes cluster with backup capabilities to Backblaze.</p>"},{"location":"longhorn/#configuration","title":"Configuration","text":""},{"location":"longhorn/#backblaze-setup","title":"Backblaze Setup","text":"<p>Back blaze is a cheap distributed file system storage solution, compatible with S3. It's used for backups for DBs, container registry and anything that needed to be backed up, really. </p> <p>You must configure longhorn to use your backblaze bucket as backup.</p> <ol> <li> <p>Create a Backblaze account and bucket:</p> </li> <li> <p>Sign up at Backblaze</p> </li> <li>Create a new private bucket</li> <li> <p>Generate application key with write access</p> </li> <li> <p>Configure Backblaze credentials in <code>group_vars/all/secrets.yml</code>:</p> </li> </ol> <pre><code>backblaze_key_id: \"your-backblaze-key-id\"\nbackblaze_application_key: \"your-backblaze-application-key\"\nbackblaze_bucket: \"your-backblaze-bucket\"\nbackblaze_region: \"your-region\"\n</code></pre> <p>See Example secrets file for details.</p>"},{"location":"longhorn/#managing-backups","title":"Managing Backups","text":"<p>The ansible scripts will automatically configure automatic weekly backups for you, see the corresponding section in longhorn role for details.</p>"},{"location":"longhorn/#accessing-the-ui","title":"Accessing the UI","text":"<ol> <li>Set up port forwarding:</li> </ol> <pre><code>kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80\n</code></pre> <ol> <li>Access the UI at http://localhost:8080</li> </ol>"},{"location":"longhorn/#ui-features","title":"UI Features","text":"<ul> <li>Volume management</li> <li>Backup scheduling</li> <li>Performance monitoring</li> <li>Disaster recovery</li> <li>Volume snapshots</li> </ul>"},{"location":"longhorn/#backup-configuration","title":"Backup Configuration","text":""},{"location":"longhorn/#automated-backups","title":"Automated Backups","text":"<ol> <li>Configure backup schedule in Longhorn UI:</li> <li>Go to Settings &gt; Backup</li> <li>Set backup interval</li> <li>Configure retention policy</li> </ol>"},{"location":"longhorn/#backup-verification","title":"Backup Verification","text":"<ol> <li>Check backup status:</li> <li> <p>By using the UI, as explained above</p> </li> <li> <p>Verify backup in Backblaze:</p> </li> <li>Log in to Backblaze</li> <li>Navigate to your bucket</li> <li>Check backup files</li> </ol>"},{"location":"longhorn/#troubleshooting","title":"Troubleshooting","text":""},{"location":"longhorn/#common-issues","title":"Common Issues","text":"<ol> <li>Backup Failures</li> <li>Check Backblaze credentials</li> <li>Verify network connectivity</li> <li>Check Longhorn logs:      <code>bash      kubectl logs -n longhorn-system -l app=longhorn-manager</code></li> </ol> <p>If you fill variables right, backups should be all automatically configured for you, it should be plug and play.      </p>"},{"location":"mailu_smtp/","title":"Configure SMTP (Mailu) for Your Self-Hosted SaaS","text":"<p>This guide explains how to enable and configure SMTP email sending for your self-hosted SaaS projects using Mailu, and how to set up the required DNS records for reliable email delivery.</p>"},{"location":"mailu_smtp/#1-enable-mailu-globally","title":"1. Enable Mailu Globally","text":"<p>In your <code>group_vars/all/secrets.yml</code> (or <code>secrets.example.yml</code>), set the following variable to enable Mailu for all projects:</p> <pre><code>install_mailu_for_smtp: true\n</code></pre>"},{"location":"mailu_smtp/#2-configure-smtp-per-project","title":"2. Configure SMTP Per Project","text":"<p>Each project in the <code>self_saas_projects</code> array can have its own SMTP configuration. Example:</p> <pre><code>self_saas_projects:\n  - name: example-self-hosted-saas-app\n    domain: example.mvalle.com\n    smtp:\n      enabled: true  # Set to false to skip Mailu config for this project\n      domain: smtp.example.mvalle.com\n      user: noreply@example.mvalle.com\n      password: \"your_smtp_password\"\n  - name: another-app\n    domain: another.example.com\n    smtp:\n      enabled: true\n      domain: smtp.another.example.com\n      user: noreply@another.example.com\n      password: \"your_smtp_password\"\n</code></pre> <ul> <li>Only projects with <code>mailu.enabled: true</code> will be included in the Mailu configuration.</li> <li>The <code>domain</code> under <code>smtp</code> should be the SMTP hostname you want to use for that project.</li> <li>The <code>user</code> and <code>password</code> are the credentials Mailu will use for sending mail from that project.</li> </ul>"},{"location":"mailu_smtp/#3-dns-requirements","title":"3. DNS Requirements","text":"<p>For each domain you want to send (and optionally receive) email from, you must configure DNS records:</p>"},{"location":"mailu_smtp/#mx-records","title":"MX Records","text":"<p>Set the MX record for each domain to point to its SMTP hostname:</p> <pre><code>example.mvalle.com.   IN MX 10 smtp.example.mvalle.com.\nanother.example.com.  IN MX 10 smtp.another.example.com.\n</code></pre>"},{"location":"mailu_smtp/#a-or-cname-records","title":"A or CNAME Records","text":"<p>Each SMTP hostname must resolve to the public IP address of your Mailu server (the LoadBalancer or Ingress IP):</p> <pre><code>smtp.example.mvalle.com.   IN A &lt;mailu-public-ip&gt;\nsmtp.another.example.com.  IN A &lt;mailu-public-ip&gt;\n</code></pre>"},{"location":"mailu_smtp/#spf-dkim-and-dmarc","title":"SPF, DKIM, and DMARC","text":"<p>For best deliverability, add these records for each domain: - SPF: Authorizes your Mailu server to send mail for your domain. - DKIM: Cryptographically signs your emails (Mailu can generate the DKIM key). - DMARC: Tells receiving servers how to handle unauthenticated mail.</p> <p>Consult the Mailu admin UI or documentation for the exact DKIM record to add.</p>"},{"location":"mailu_smtp/#reverse-dns-ptr","title":"Reverse DNS (PTR)","text":"<p>The public IP used by Mailu should have a PTR record (reverse DNS) matching one of your SMTP hostnames (e.g., <code>smtp.example.mvalle.com</code>).</p>"},{"location":"mailu_smtp/#4-example","title":"4. Example","text":"<p>If you have two projects, your DNS zone might look like:</p> <pre><code>example.mvalle.com.   IN MX 10 smtp.example.mvalle.com.\nsmtp.example.mvalle.com.   IN A 203.0.113.10\nanother.example.com.  IN MX 10 smtp.another.example.com.\nsmtp.another.example.com.  IN A 203.0.113.10\n</code></pre>"},{"location":"mailu_smtp/#5-additional-notes","title":"5. Additional Notes","text":"<ul> <li>You only need to deploy Mailu once; it will handle all enabled domains.</li> <li>If you want to disable Mailu for a specific project, set <code>mailu.enabled: false</code> for that project.</li> <li>Always test your email deliverability and check spam folders after setup.</li> </ul> <p>For more details, see the Mailu documentation.</p>"},{"location":"mailu_smtp/#6-accessing-the-mailu-admin-ui","title":"6. Accessing the Mailu Admin UI","text":"<p>To access the Mailu Admin web interface locally, use kubectl port-forward:</p> <pre><code>kubectl -n mailu port-forward svc/mailu-admin 8082:80\n</code></pre> <p>Then open http://localhost:8082 in your browser. Login with your admin credentials.</p>"},{"location":"monitoring/","title":"Monitoring (Prometheus &amp; Grafana)","text":"<p>This stack provides cluster monitoring and visualization using Prometheus and Grafana.</p>"},{"location":"monitoring/#configuration","title":"Configuration","text":"<p>All configuration is handled via Ansible variables in <code>roles/monitoring/defaults/main.yml</code> or can be overridden in your inventory.</p> <p>Key variables:</p> <pre><code>monitoring_namespace: monitoring\n\nprometheus_enabled: true\nprometheus_storage_size: 5Gi\nprometheus_storage_class: longhorn-single-replica\nprometheus_alertmanager_enabled: false\n\ngrafana_enabled: true\ngrafana_storage_size: 5Gi\ngrafana_storage_class: longhorn-single-replica\ngrafana_admin_user: admin\ngrafana_admin_password: admin\n</code></pre>"},{"location":"monitoring/#accessing-the-grafana-ui","title":"Accessing the Grafana UI","text":"<ol> <li>Set up port forwarding:</li> </ol> <p><code>bash    kubectl -n monitoring port-forward svc/grafana 3000:80</code></p> <ol> <li> <p>Access the UI at http://localhost:3000</p> </li> <li> <p>Login with:</p> </li> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code>    (or your custom credentials if you changed them)</li> </ol>"},{"location":"monitoring/#grafana-features","title":"Grafana Features","text":"<ul> <li>Pre-built dashboards for Kubernetes and Prometheus metrics</li> <li>Custom dashboard creation</li> <li>Alerting and notifications</li> <li>Data source management</li> </ul>"},{"location":"monitoring/#accessing-the-prometheus-ui","title":"Accessing the Prometheus UI","text":"<ol> <li>Set up port forwarding:</li> </ol> <p><code>bash    kubectl -n monitoring port-forward svc/prometheus-server 9090:80</code></p> <ol> <li>Access the UI at http://localhost:9090</li> </ol>"},{"location":"monitoring/#prometheus-features","title":"Prometheus Features","text":"<ul> <li>Query metrics with PromQL</li> <li>Explore time-series data</li> <li>Configure alerting rules (if enabled)</li> </ul>"},{"location":"monitoring/#troubleshooting","title":"Troubleshooting","text":""},{"location":"monitoring/#common-issues","title":"Common Issues","text":"<ol> <li>Grafana/Prometheus not accessible</li> <li>Ensure pods are running: <code>bash      kubectl get pods -n monitoring</code></li> <li> <p>Check port-forward command and local firewall</p> </li> <li> <p>No data in dashboards</p> </li> <li>Check Prometheus targets in its UI</li> <li> <p>Ensure node exporters and kube-state-metrics are running</p> </li> <li> <p>Login issues</p> </li> <li>Verify credentials in your Ansible variables</li> </ol> <p>If you configured variables correctly, monitoring should be ready to use after deployment.</p>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>Before installing Self-Host SaaS K3s, ensure you have the following requirements in place.</p>"},{"location":"prerequisites/#server-requirements","title":"Server Requirements","text":""},{"location":"prerequisites/#hardware","title":"Hardware","text":"<ul> <li>CPU: 2+ cores</li> <li>RAM: 4GB minimum (8GB recommended)</li> <li>Storage: 20GB minimum (SSD recommended)</li> <li>Network: 1Gbps connection</li> </ul>"},{"location":"prerequisites/#software","title":"Software","text":"<ul> <li>Ubuntu 24.04 LTS</li> <li>SSH access with sudo privileges</li> <li>Static IP address (public or private)</li> </ul>"},{"location":"prerequisites/#local-machine-setup","title":"Local Machine Setup","text":""},{"location":"prerequisites/#required-software","title":"Required Software","text":"<ol> <li> <p>Ansible <code>bash    sudo apt update    sudo apt install -y ansible</code></p> </li> <li> <p>Python Packages <code>bash    pip install kubernetes openshift</code></p> </li> <li> <p>Helm 3 <code>bash    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</code></p> </li> </ol>"},{"location":"prerequisites/#ssh-key-setup","title":"SSH Key Setup","text":"<ol> <li> <p>Generate SSH key if you don't have one:    <code>bash    ssh-keygen -t rsa -b 4096</code></p> </li> <li> <p>Add your public key to the server:    <code>bash    ssh-copy-id ubuntu@YOUR_SERVER_PUBLIC_IP</code></p> </li> </ol>"},{"location":"prerequisites/#network-requirements","title":"Network Requirements","text":""},{"location":"prerequisites/#dns-configuration","title":"DNS Configuration","text":"<ol> <li>Set up DNS records for your domain (optional):    <code>registry.yourdomain.com -&gt; YOUR_SERVER_IP    *.yourdomain.com -&gt; YOUR_SERVER_IP</code></li> </ol>"},{"location":"prerequisites/#verification","title":"Verification","text":"<p>Before proceeding with installation, verify:</p> <ol> <li>Server meets requirements:    <code>bash    ansible all -i inventory.yml -m shell -a \"free -h &amp;&amp; df -h &amp;&amp; nproc\"</code></li> </ol>"},{"location":"security/","title":"Security Configuration","text":"<p>Self-Host SaaS K3s applies several security best practices by default. You can customize these settings by editing the relevant variables in your <code>group_vars/all/secrets.yml</code> file.</p>"},{"location":"security/#key-security-variables","title":"Key Security Variables","text":"<p>Set these variables in <code>secrets.yml</code> to control security features:</p> <ul> <li> <p><code>ssh_public_key_to_authorize_on_target</code>:   Path to the SSH public key on your control machine. This key will be authorized for the <code>ubuntu</code> user on the server. Example: <code>yaml   ssh_public_key_to_authorize_on_target: \"{{ lookup('env', 'HOME') }}/.ssh/id_rsa.pub\"</code></p> </li> <li> <p><code>firewall_open_k3s_ports</code>:   Set to <code>true</code> to allow access to the K3s API port (default: 6443) from the internet. Default: <code>false</code> (K3s API is not exposed externally).</p> </li> </ul>"},{"location":"security/#firewall-rules","title":"Firewall Rules","text":"<p>The firewall is managed using UFW and is configured with the following rules:</p> <ul> <li>Default policy: Deny all incoming connections.</li> <li>SSH: </li> <li>Only allows connections on the SSH port (default: 2222).</li> <li>Password authentication is disabled; only key-based authentication is permitted.</li> <li>HTTP/HTTPS: </li> <li>Ports 80 (HTTP) and 443 (HTTPS) are open to allow web traffic.</li> <li>K3s API: </li> <li>Port 6443 is only open if <code>firewall_open_k3s_ports</code> is set to <code>true</code>.</li> </ul>"},{"location":"security/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Always use SSH keys for authentication.</li> <li>Change the default SSH port if needed by editing <code>ansible_port</code> in your inventory, but the firewall will only allow the port specified in the playbook.</li> <li>Keep your SSH private keys secure and never share them.</li> <li>Only set <code>firewall_open_k3s_ports</code> to <code>true</code> if you need to access the Kubernetes API remotely, and consider restricting access by IP.</li> </ul> <p>For a full list of configurable secrets, see group_vars/all/secrets.example.yml.</p>"},{"location":"security/#ssh-tunnels","title":"SSH tunnels","text":"<p>For security reasons, most UIs and services in the cluster are not directly exposed to the internet. Instead, they are accessed through SSH tunnels, which provide a secure way to access these services. This approach ensures that:</p> <ol> <li>Services are only accessible to authorized users with SSH access</li> <li>All traffic is encrypted through the SSH connection</li> <li>No additional ports need to be opened on the firewall</li> </ol> <p>For detailed instructions on setting up and automating SSH tunnels, including various methods like using <code>autossh</code>, systemd services, and SSH config, please refer to our SSH Tunnel Guide.</p>"},{"location":"server-preparation/","title":"Server Preparation","text":"<p>This guide explains how to prepare your server using cloud-init before running the Ansible playbook.</p>"},{"location":"server-preparation/#cloud-init-configuration","title":"Cloud-Init Configuration","text":"<p>The cloud-init configuration is defined in cloud-init.yml. This configuration: - Creates the ubuntu user with sudo access - Changes SSH port to 2222 - Disables password authentication - Installs required packages - Configures the firewall</p>"},{"location":"server-preparation/#using-cloud-init","title":"Using Cloud-Init","text":""},{"location":"server-preparation/#1-create-the-configuration","title":"1. Create the Configuration","text":"<ol> <li> <p>Copy the cloud-init configuration:    <code>bash    cp cloud-init.yml server-init.yml</code></p> </li> <li> <p>Edit <code>server-init.yml</code>:</p> </li> <li>Replace <code>YOUR_SSH_PUBLIC_KEY</code> with your actual SSH public key</li> </ol>"},{"location":"server-preparation/#2-apply-cloud-init","title":"2. Apply Cloud-Init","text":""},{"location":"server-preparation/#for-ovh-recommended","title":"For OVH (Recommended):","text":"<ol> <li>Go to OVH Control Panel &gt; Dedicated Servers</li> <li>Select your server</li> <li>Go to \"Network\" tab</li> <li>Under \"User data\", paste the contents of <code>server-init.yml</code></li> <li>Click \"Apply\"</li> </ol>"},{"location":"server-preparation/#for-other-providers","title":"For Other Providers:","text":"<ul> <li>DigitalOcean: Use \"User data\" field when creating a Droplet</li> <li>AWS: Use \"User data\" field when launching an EC2 instance</li> <li>Other providers: Check their documentation for cloud-init support</li> </ul>"},{"location":"server-preparation/#verifying-the-setup","title":"Verifying the Setup","text":"<ol> <li> <p>Wait for the server to initialize (usually 1-2 minutes)</p> </li> <li> <p>Connect to the server:    <code>bash    ssh -p 2222 ubuntu@YOUR_SERVER_IP</code></p> </li> <li> <p>Verify the configuration:    ```bash    # Check SSH port    sudo netstat -tuln | grep 2222</p> </li> </ol> <p># Check firewall    sudo ufw status    ```</p>"},{"location":"server-preparation/#troubleshooting","title":"Troubleshooting","text":"<p>If you cannot connect to the server: 1. Verify the SSH port (2222) 2. Check if the server is running 3. Verify your SSH key is correct 4. Check cloud-init logs: <code>sudo cat /var/log/cloud-init-output.log</code></p>"},{"location":"server-preparation/#next-steps","title":"Next Steps","text":"<p>After the server is prepared: 1. Update your inventory file with the new SSH port 2. Proceed with the installation guide </p>"},{"location":"ssh-tunnel/","title":"SSH Tunnel Automation","text":"<p>This guide provides various methods to automate the SSH tunnel for accessing your K3s cluster.</p>"},{"location":"ssh-tunnel/#basic-ssh-tunnel","title":"Basic SSH Tunnel","text":"<p>The basic SSH tunnel command:</p> <pre><code>ssh -N -L 6443:localhost:6443 ubuntu@YOUR_PUBLIC_IP\n</code></pre>"},{"location":"ssh-tunnel/#automation-options","title":"Automation Options","text":""},{"location":"ssh-tunnel/#1-using-autossh-recommended","title":"1. Using autossh (Recommended)","text":"<p><code>autossh</code> provides automatic reconnection if the connection drops:</p> <pre><code># Install autossh\nsudo apt-get install autossh\n\n# Create a persistent tunnel\nautossh -M 0 -N -L 6443:localhost:6443 ubuntu@YOUR_PUBLIC_IP\n</code></pre>"},{"location":"ssh-tunnel/#2-systemd-service","title":"2. Systemd Service","text":"<p>Create a systemd service for automatic tunnel management:</p> <pre><code># Create service file\nsudo nano /etc/systemd/system/k3s-tunnel.service\n</code></pre> <p>Add the following content:</p> <pre><code>[Unit]\nDescription=K3s SSH Tunnel\nAfter=network.target\n\n[Service]\nUser=YOUR_USERNAME\nExecStart=/usr/bin/autossh -M 0 -N -L 6443:localhost:6443 ubuntu@YOUR_PUBLIC_IP\nRestart=always\nRestartSec=3\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start the service:</p> <pre><code>sudo systemctl enable k3s-tunnel\nsudo systemctl start k3s-tunnel\n</code></pre>"},{"location":"ssh-tunnel/#3-shell-script","title":"3. Shell Script","text":"<p>Create a simple shell script for manual tunnel management:</p> <pre><code># Create tunnel script\nnano ~/k3s-tunnel.sh\n</code></pre> <p>Add the following content:</p> <pre><code>#!/bin/bash\nssh -N -L 6443:localhost:6443 ubuntu@YOUR_PUBLIC_IP\n</code></pre> <p>Make it executable:</p> <pre><code>chmod +x ~/k3s-tunnel.sh\n</code></pre>"},{"location":"ssh-tunnel/#4-ssh-config","title":"4. SSH Config","text":"<p>Configure SSH for easier connection management:</p> <pre><code># Add to ~/.ssh/config\nnano ~/.ssh/config\n</code></pre> <p>Add the following configuration:</p> <pre><code>Host k3s-tunnel\n    HostName YOUR_PUBLIC_IP\n    User ubuntu\n    LocalForward 6443 localhost:6443\n    ServerAliveInterval 60\n    ServerAliveCountMax 3\n</code></pre> <p>Then you can simply use:</p> <pre><code>ssh k3s-tunnel\n</code></pre>"},{"location":"ssh-tunnel/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ssh-tunnel/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Drops</li> <li>Check your network connection</li> <li>Verify the server is running</li> <li> <p>Check SSH server logs: <code>journalctl -u ssh</code></p> </li> <li> <p>Permission Denied</p> </li> <li>Verify SSH key permissions: <code>chmod 600 ~/.ssh/id_rsa</code></li> <li> <p>Check SSH config permissions: <code>chmod 600 ~/.ssh/config</code></p> </li> <li> <p>Port Already in Use</p> </li> <li>Check if port 6443 is already in use: <code>netstat -tuln | grep 6443</code></li> <li>Kill existing process: <code>kill $(lsof -t -i:6443)</code></li> </ol>"},{"location":"ssh-tunnel/#monitoring","title":"Monitoring","text":"<ol> <li>Check Tunnel Status    ```bash    # For systemd service    systemctl status k3s-tunnel</li> </ol> <p># For autossh    ps aux | grep autossh    ```</p> <ol> <li>View Logs    ```bash    # Systemd service logs    journalctl -u k3s-tunnel -f</li> </ol> <p># SSH logs    tail -f /var/log/auth.log    ```</p>"},{"location":"ssh-tunnel/#security-considerations","title":"Security Considerations","text":"<ol> <li>SSH Key Management</li> <li>Use strong SSH keys</li> <li>Regularly rotate keys</li> <li> <p>Use key-based authentication only</p> </li> <li> <p>Network Security</p> </li> <li>Restrict SSH access to specific IPs</li> <li>Use non-standard SSH port</li> <li> <p>Enable fail2ban</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor failed login attempts</li> <li>Set up alerts for suspicious activity</li> <li>Regular security audits </li> </ol>"}]}